{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, Embedding, LSTM, Linear, CrossEntropyLoss, NLLLoss\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import katakana.encoding as encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the joined titles dataset similar to the [TensorFlow's version](./Writing%20Katakana%20using%20Sequence-to-Sequence%20in%20TensorFlow.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dorogobuzh', 'gail hopkins', 'novatek']\n",
      "['ドロゴブージ', 'ゲイル・ホプキンス', 'ノヴァテク']\n",
      "training size 64356\n",
      "validation size 10726\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/joined_titles.csv', header=None)\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "\n",
    "data_input = [s.lower() for s in data[0]]\n",
    "data_output = [s.lower() for s in data[1]]\n",
    "print(data_input[0:3])\n",
    "print(data_output[0:3])\n",
    "\n",
    "data_size = len(data)\n",
    "training_split_index = int(data_size*60/100)\n",
    "validation_split_index = int(data_size*70/100)\n",
    "\n",
    "# We will use the first 0-60th %-tile (60%) of data for the training\n",
    "training_input  = data_input[:training_split_index]\n",
    "training_output = data_output[:training_split_index]\n",
    "\n",
    "# We will use the first 60-70th %-tile (10%) of data for the training\n",
    "validation_input = data_input[training_split_index:validation_split_index]\n",
    "validation_output = data_output[training_split_index:validation_split_index]\n",
    "\n",
    "print('training size', len(training_input))\n",
    "print('validation size', len(validation_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also reuse the data encoding and transform already written in `katakana/encoding.py`. (See. [Writing Katakana using Sequence-to-Sequence in TensorFlow](./Writing%20Katakana%20using%20Sequence-to-Sequence%20in%20TensorFlow.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English character dict size: 54\n",
      "Katakana character dict size: 89\n"
     ]
    }
   ],
   "source": [
    "english_encoding, english_decoding, english_dict_size = encoding.build_characters_encoding(data_input)\n",
    "japanese_encoding, japanese_decoding, japanese_dict_size = encoding.build_characters_encoding(data_output)\n",
    "\n",
    "print('English character dict size:', english_dict_size)\n",
    "print('Katakana character dict size:', japanese_dict_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_training_input (64356, 20)\n",
      "encoded_training_output (64356, 20)\n",
      "encoded_validation_input (10726, 20)\n",
      "encoded_validation_output (10726, 20)\n"
     ]
    }
   ],
   "source": [
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "\n",
    "encoded_training_input = encoding.transform(\n",
    "    english_encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = encoding.transform(\n",
    "    japanese_encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_training_input', encoded_training_input.shape)\n",
    "print('encoded_training_output', encoded_training_output.shape)\n",
    "\n",
    "encoded_validation_input = encoding.transform(\n",
    "    english_encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = encoding.transform(\n",
    "    japanese_encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_validation_input', encoded_validation_input.shape)\n",
    "print('encoded_validation_output', encoded_validation_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 24 12 24  7 24 18 35 47 28  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_training_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85 50 17 65 21 58  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_training_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The encoder consists of [Embedding](https://pytorch.org/docs/stable/nn.html#embedding) and [LSTM](https://pytorch.org/docs/stable/nn.html#lstm). \n",
    "\n",
    "It first embeds each character input into a vector. Then, it feeds the embeded input into the LSTM. After all characters have been processed, we take the final LSTM output as the encoder output.\n",
    "\n",
    "Note: we need to set `batch_first=True` to make Pytorch's LSTM taking input with dimensions (batch_size, sequnece_size, vector_size) similar to TensorFlow's LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embedding): Embedding(54, 64)\n",
      "  (lstm): LSTM(64, 64, batch_first=True)\n",
      ")\n",
      "encoder_input torch.Size([2, 20]) torch.int64\n",
      "encoder_output torch.Size([2, 64]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "class Encoder(Module):\n",
    "    \n",
    "    def __init__(self, input_dict_size=english_dict_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(input_dict_size, 64)\n",
    "        self.lstm = LSTM(64, 64, batch_first=True)\n",
    "\n",
    "    def forward(self, encoder_input_sequences):\n",
    "        embedded = self.embedding(encoder_input_sequences)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        return output[:, -1]\n",
    "    \n",
    "encoder = Encoder()\n",
    "print(encoder)\n",
    "\n",
    "encoder_input = torch.tensor(encoded_training_input[:2], dtype=torch.long)\n",
    "encoder_output = encoder(encoder_input)\n",
    "print('encoder_input', encoder_input.shape, encoder_input.dtype)\n",
    "print('encoder_output', encoder_output.shape, encoder_output.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The encoder consists of [Embedding](https://pytorch.org/docs/stable/nn.html#embedding), [LSTM](https://pytorch.org/docs/stable/nn.html#lstm), and [Linear](https://pytorch.org/docs/stable/nn.html#linear). \n",
    "\n",
    "We train decoder to output the next Katakana character in the sequence. The decoder inputs are Katakana sequences and the output from the encoder.\n",
    "\n",
    "Similar to the encoder, the decoder embeds input the sequence and pass the embeded sequence to LSTM. However, this time, we initialize the LSTM's state with encoder's output. The LSTM's output are then passed into the linear layer to produce the final output.\n",
    "\n",
    "Note: We don't apply Softmax activation to the final output to make it easier to apply `CrossEntropyLoss` (see \"Training the model\"). Applying the Softmax also won't change the result when we use the decoder to generate the output greedily (see \"Testing the model\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (embedding): Embedding(89, 64)\n",
      "  (lstm): LSTM(64, 64, batch_first=True)\n",
      "  (linear): Linear(in_features=64, out_features=89, bias=True)\n",
      ")\n",
      "decoder_input torch.Size([2, 20]) torch.int64\n",
      "decoder_output torch.Size([2, 20, 89]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "class Decoder(Module):\n",
    "    \n",
    "    def __init__(self, output_dict_size=japanese_dict_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(output_dict_size, 64)\n",
    "        self.lstm = LSTM(64, 64, batch_first=True)\n",
    "        self.linear = Linear(64, output_dict_size)\n",
    "\n",
    "    def forward(self, encoder_output, decoder_input_sequence):\n",
    "        encoder_output = encoder_output.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.embedding(decoder_input_sequence)\n",
    "        output, _ = self.lstm(embedded, [encoder_output, encoder_output])\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "decoder = Decoder()\n",
    "print(decoder)\n",
    "\n",
    "decoder_input = torch.tensor(encoded_training_output[:2], dtype=torch.long)\n",
    "decoder_output = decoder(encoder_output, decoder_input)\n",
    "print('decoder_input', decoder_input.shape, decoder_input.dtype)\n",
    "print('decoder_output', decoder_output.shape, decoder_output.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(54, 64)\n",
      "    (lstm): LSTM(64, 64, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(89, 64)\n",
      "    (lstm): LSTM(64, 64, batch_first=True)\n",
      "    (linear): Linear(in_features=64, out_features=89, bias=True)\n",
      "  )\n",
      ")\n",
      "model_output tensor([[[ 0.0956,  0.0912, -0.1650,  ...,  0.1132,  0.0783, -0.0100],\n",
      "         [ 0.0156,  0.1449, -0.1823,  ...,  0.0277,  0.1391,  0.0440],\n",
      "         [-0.0021,  0.0035, -0.1248,  ...,  0.0535,  0.1420,  0.0121],\n",
      "         ...,\n",
      "         [ 0.0977,  0.0540,  0.0132,  ...,  0.1889,  0.0122,  0.1002],\n",
      "         [ 0.0979,  0.0544,  0.0131,  ...,  0.1889,  0.0120,  0.1003],\n",
      "         [ 0.0981,  0.0546,  0.0130,  ...,  0.1890,  0.0119,  0.1004]],\n",
      "\n",
      "        [[-0.0080,  0.0080,  0.0017,  ...,  0.1097,  0.1302, -0.0538],\n",
      "         [ 0.0537,  0.0877, -0.1111,  ...,  0.1541,  0.1805, -0.2501],\n",
      "         [-0.0922,  0.1256, -0.0422,  ...,  0.0806,  0.0285, -0.1616],\n",
      "         ...,\n",
      "         [ 0.0981,  0.0526,  0.0158,  ...,  0.1842,  0.0120,  0.1012],\n",
      "         [ 0.0981,  0.0532,  0.0151,  ...,  0.1854,  0.0121,  0.1010],\n",
      "         [ 0.0981,  0.0536,  0.0145,  ...,  0.1863,  0.0122,  0.1009]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, encoder_input_sequences, decoder_input_sequences):\n",
    "        encoder_output = self.encoder(encoder_input_sequences)\n",
    "        decoder_output = self.decoder(encoder_output, decoder_input_sequences)\n",
    "        return decoder_output\n",
    "\n",
    "model = Seq2Seq()\n",
    "print(model)\n",
    "\n",
    "encoder_input = torch.tensor(encoded_training_input[:2], dtype=torch.long)\n",
    "decoder_input = torch.tensor(encoded_training_output[:2], dtype=torch.long)\n",
    "model_output = model(encoder_input, decoder_input)\n",
    "print('model_output', model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input [[13 24 12 24  7 24 18 35 47 28  0  0  0  0  0  0  0  0  0  0]]\n",
      "decoder input [[ 1 85 50 17 65 21 58  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "(expected) decoder output [[85 50 17 65 21 58  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Encoder Input\n",
    "training_encoder_input = encoded_training_input\n",
    "training_decoder_output = encoded_training_output\n",
    "\n",
    "# Decoder Input (need padding py START_CHAR_CODE)\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = encoding.CHAR_CODE_START\n",
    "\n",
    "print('encoder input', training_encoder_input[:1])\n",
    "print('decoder input', training_decoder_input[:1])\n",
    "print('(expected) decoder output', training_decoder_output[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input [[26 38 16 28 24 38 34 22 19 24 12 12 24 37  0  0  0  0  0  0]]\n",
      "decoder input [[ 1  7 11 19 52 21  8 64 50 21  0  0  0  0  0  0  0  0  0  0]]\n",
      "(expected) decoder output [[ 7 11 19 52 21  8 64 50 21  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = encoding.CHAR_CODE_START\n",
    "validation_decoder_output = encoded_validation_output\n",
    "\n",
    "print('encoder input', validation_encoder_input[:1])\n",
    "print('decoder input', validation_decoder_input[:1])\n",
    "print('(expected) decoder output', validation_decoder_output[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, \n",
    "                batch_size=64, \n",
    "                criterion=CrossEntropyLoss(),\n",
    "                encoder_input=training_encoder_input,\n",
    "                decoder_input=training_decoder_input,\n",
    "                decoder_output=training_decoder_output):\n",
    "    \n",
    "    # re-shuffle the training_data:\n",
    "    permutation = np.random.permutation(encoder_input.shape[0])\n",
    "    encoder_input = encoder_input[permutation]\n",
    "    decoder_input = decoder_input[permutation]\n",
    "    decoder_output = decoder_output[permutation]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    iteration_count = 0\n",
    "    \n",
    "    for begin_index in range(0, len(encoder_input), batch_size):    \n",
    "        end_index = begin_index + batch_size\n",
    "        iteration_count += 1\n",
    "        \n",
    "        encoder_input_step = torch.tensor(encoder_input[begin_index:end_index])\n",
    "        decoder_input_step = torch.tensor(decoder_input[begin_index:end_index])\n",
    "        decoder_output_step = torch.tensor(decoder_output[begin_index:end_index])\n",
    "        \n",
    "        # If training on GPU...\n",
    "        # model.cuda()\n",
    "        # encoder_input_step = encoder_input_step.cuda()\n",
    "        # decoder_input_step = decoder_input_step.cuda()\n",
    "        # decoder_output_step = decoder_output_step.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(encoder_input_step, decoder_input_step)\n",
    "        target = decoder_output_step.view(-1)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / iteration_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, \n",
    "             criterion=CrossEntropyLoss(),\n",
    "             encoder_input=validation_encoder_input,\n",
    "             decoder_input=validation_decoder_input,\n",
    "             decoder_output=validation_decoder_output):\n",
    "    \n",
    "    encoder_input = torch.tensor(encoder_input)\n",
    "    decoder_input = torch.tensor(decoder_input)\n",
    "    decoder_output = torch.tensor(decoder_output)\n",
    "\n",
    "    # If training on GPU...\n",
    "    # model.cuda()\n",
    "    # encoder_input = encoder_input.cuda()\n",
    "    # decoder_input = decoder_input.cuda()\n",
    "    # decoder_output = decoder_output.cuda()\n",
    "    \n",
    "    output = model(encoder_input, decoder_input)\n",
    "    \n",
    "    target = decoder_output.view(-1)\n",
    "    output = output.view(-1, output.shape[-1])\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "> Training Loss 1.5449128637967953\n",
      "Epoch 2\n",
      "> Training Loss 1.2714332525820191\n",
      "Epoch 3\n",
      "> Training Loss 1.1162601499031362\n",
      "> Validation Loss 1.0606048107147217\n",
      "Epoch 4\n",
      "> Training Loss 1.008581491042795\n",
      "Epoch 5\n",
      "> Training Loss 0.9326041009032703\n",
      "Epoch 6\n",
      "> Training Loss 0.8779584057406924\n",
      "> Validation Loss 0.8624294996261597\n",
      "Epoch 7\n",
      "> Training Loss 0.8386473264656295\n",
      "Epoch 8\n",
      "> Training Loss 0.8072047768601366\n",
      "Epoch 9\n",
      "> Training Loss 0.7812739739242652\n",
      "> Validation Loss 0.7808693051338196\n",
      "Epoch 10\n",
      "> Training Loss 0.7585053891832264\n",
      "Epoch 11\n",
      "> Training Loss 0.7382411765412356\n",
      "Epoch 12\n",
      "> Training Loss 0.7207917072782459\n",
      "> Validation Loss 0.7323684692382812\n",
      "Epoch 13\n",
      "> Training Loss 0.7055101474047181\n",
      "Epoch 14\n",
      "> Training Loss 0.6918286189407288\n",
      "Epoch 15\n",
      "> Training Loss 0.6797983193729317\n",
      "> Validation Loss 0.6897333860397339\n",
      "Epoch 16\n",
      "> Training Loss 0.669035366379006\n",
      "Epoch 17\n",
      "> Training Loss 0.6592021589250735\n",
      "Epoch 18\n",
      "> Training Loss 0.6500768760148858\n",
      "> Validation Loss 0.669581413269043\n",
      "Epoch 19\n",
      "> Training Loss 0.6419586258427523\n",
      "Epoch 20\n",
      "> Training Loss 0.6340698531678847\n",
      "Epoch 21\n",
      "> Training Loss 0.6275975768182197\n",
      "> Validation Loss 0.647014856338501\n",
      "Epoch 22\n",
      "> Training Loss 0.6204441244275148\n",
      "Epoch 23\n",
      "> Training Loss 0.6142950773120638\n",
      "Epoch 24\n",
      "> Training Loss 0.608434274111306\n",
      "> Validation Loss 0.6333272457122803\n",
      "Epoch 25\n",
      "> Training Loss 0.6028112002739139\n",
      "Epoch 26\n",
      "> Training Loss 0.5975572661483738\n",
      "Epoch 27\n",
      "> Training Loss 0.5931276173643752\n",
      "> Validation Loss 0.6151658892631531\n",
      "Epoch 28\n",
      "> Training Loss 0.588091174159088\n",
      "Epoch 29\n",
      "> Training Loss 0.5842529415610296\n",
      "Epoch 30\n",
      "> Training Loss 0.5797584179203743\n",
      "> Validation Loss 0.6064770817756653\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, optimizer, n_epoch=30, validate_every_n_epoach=3):\n",
    "    \n",
    "    for i in range(1, n_epoch + 1):\n",
    "        print('Epoch %i' % i)\n",
    "        \n",
    "        loss = train_epoch(model, optimizer)\n",
    "        print('> Training Loss', loss)\n",
    "        \n",
    "        if i % validate_every_n_epoach == 0:\n",
    "            validation_loss = validate(model)\n",
    "            print('> Validation Loss', validation_loss)\n",
    "        \n",
    "model = Seq2Seq()\n",
    "optimizer = Adam(model.parameters())\n",
    "train_model(model, optimizer, n_epoch=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "During the testing or after deploy the model, to generate the output we will use \"greedy\" generating approach, which is generating one output at a time by maximize softmax score and feed the output back as the next decoder input character. \n",
    "\n",
    "We won't use [beam-search decoding](https://www.quora.com/Why-is-beam-search-required-in-sequence-to-sequence-transduction-using-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 33, 47,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_output(input_sequence):\n",
    "    \n",
    "    decoder_input = np.zeros(shape=(len(input_sequence), OUTPUT_LENGTH), dtype='int')\n",
    "    decoder_input[:,0] = encoding.CHAR_CODE_START\n",
    "    \n",
    "    encoder_input = torch.tensor(input_sequence)\n",
    "    decoder_input = torch.tensor(decoder_input)\n",
    "    \n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        model.cpu()\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        output = output.argmax(dim=2)\n",
    "        decoder_input[:,i] = output[:,i-1]\n",
    "        \n",
    "    return decoder_input[:,1:].detach().numpy()\n",
    "\n",
    "def to_katakana(text):\n",
    "    input_sequence = encoding.transform(english_encoding, [text.lower()], 20)\n",
    "    output_sequence = generate_output(input_sequence)\n",
    "    return encoding.decode(japanese_decoding, output_sequence[0])\n",
    "\n",
    "generate('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is trained correctly, typical names should be translate correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James ジェームズ\n",
      "John ジョン\n",
      "Robert ロベルト\n",
      "Mary マーリー\n",
      "Patricia パトリアイシア\n",
      "Linda リンダ\n"
     ]
    }
   ],
   "source": [
    "common_american_names = ['James', 'John', 'Robert', 'Mary', 'Patricia', 'Linda']\n",
    "for name in common_american_names:\n",
    "    print(name, to_katakana(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we train the model with mostly people and places names, some English words may not be written correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コンプター\n"
     ]
    }
   ],
   "source": [
    "print(to_katakana('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タキシ\n"
     ]
    }
   ],
   "source": [
    "print(to_katakana('taxi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "バナーナ\n"
     ]
    }
   ],
   "source": [
    "print(to_katakana('banana'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
